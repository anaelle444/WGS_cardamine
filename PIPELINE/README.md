# Pipeline scripts

This is the README for the pipeline I used to analyse WGS data of 87 cardamine accessions. All the scripts are numeroted and in order of the work flow. This pipeline preprocess the sequencing data and allow for variant calling and population structure analysis.
Most of the scripts are written to use an array from 86 to 172 in a HPC environment.

**01_bbduk.sh**

- indir : input data .fastq
- outdir : output data .fastq

This script create pairs first as bbduk trim the sequence by pairs of read 1 and 2. BBduk is then used to trim the adaptor content off the sequences using the following parameters : 
- ktrim = r
- k = 25
- mink = 11
- hdist = 1
- tpe = Enabled
- tbo = Enabled

**02_index_assembly.sh**

- input : .fasta

This script is used to index the reference assembly with bwa-mem2 in preparation for the alignment of the sequencing reads to the reference.

**03_alignment.sh**

- input : .fastq
- output : .sam 

This script is used to map our sequencing reads to the reference. bwa-mem2 mem need the reference file indexed and both the reads 1 and 2 for each sample to generate a .sam file of the sequence mapped to this reference genome.

**04_sam_to_bam.sh**

- input : .sam
- output : .bam

This script create a binary version of the sam file -a bam file- using samtools view. The **-h** option includes the header in the output file which is crucial for the rest of the pipeline. The **-f 2** option filters the alignments to include only those that are "properly paired", this means that both reads of a pair are aligned and correctly oriented with respect to each other.

**05_sort_bam.sh**

- input : .bam
- output : .bam

This script uses Samtools sort to sort the reads.

**06_index_bam.sh**

- input : .bam
- output : .bam.bai

This script needs the directory where all the sorted bam files are located to index them, Samtools index generates .bam.bai files.

**07_mark_duplicates.sh**

- input : .bam
- output : .bam

This script uses MarkDuplicates from picard/2.27.5 to mark all the duplicate reads from our samples. The input bam files need to be indexed first, it will then generate a new bam file containing the reads that are marked as duplicates. 

**08_index_markdup.sh**

- input	: .bam
- output : .bam.bai

This script needs the duplicates marked bam files to index them so that we can remove the duplicate reads. Samtools index generates .bam.bai files.

**09_remove_dup.sh**

- input	: .bam
- output : .bam

This script removes the duplicate reads using samtools view -h -F. The *-h* option keeps the header in the output file and the *-F* option is the one removing the duplicates. The input bam files need to be indexed first

**10_add_RG.sh**

- input : .bam
- output : .bam

This script adds the following read group to the samples, using AddOrReplaceReadGroups from picard/2.27.5 : 
- RGID=A01097_H5LGVDRX5_1 
- RGLB=WGS_C_hirsuta_${formID} 
- RGPL=ILLUMINA 
- RGPU=H5LGVDRX5
- RGSM=WGS_C_hirsuta_${formID}

**11_haplotype_caller.sh**

- input : .bam
- output : .gvcf

HaplotypeCaller from gatk/4.3.0.0 is the first step in variant calling, creating a gvcf file that will be used later. This script first indexes the .bam files using Samtools index and then run HaplotypeCaller. The latest need the reference genome and we specify the intervals for each chromosome to reduce the time of the analysis.

**12_write_sample_map.sh**

This script write the sample map that will be used in the Data Base Import, it uses command-line argument to specify the sample map for each chromosome. 

The script needs to be run as following :
```bash
sbatch 12_write_sample_map.sh 1 #run it 8 times to create the 8 sample maps, one for each chromosome 
```

**13_GenomicsDBImport.sh**

This script is used to import our samples into data bases per chromosome. The cardamine genome has 8 chromosomes, therefore we will have 8 daba bases. GenomicsDBImport from gatk/4.3.0.0 needs the sample map generated earlier that references the path for all our samples.

**14_jointcall_GenotypeGCVF.sh**

- **input**: .gvcf
- **output**: .vcf

This script uses GenotypeGVCFs from GATK to perform joint genotyping on the gVCF files generated by the HaplotypeCaller. This step combines the individual gVCFs into a single VCF file containing the variants for all samples.

**15_hardfiltering.sh**

- **input**: .vcf
- **output**: .filtered.vcf

This script applies hard filtering to the VCF file using GATK's VariantFiltration tool. Different conditions of hardfiltering have been tested here, this folder contains a README explaining the details of the conditions.

**16_variantstotable.sh**

- **input**: .vcf
- **output**: .tsv

This script converts the filtered VCF file into a tab-separated values (TSV) file for easier analysis, using VariantsToTables from gatk/4.3.0.0. It extracts relevant columns and formats them for further analysis.

**17_merge.sh**

- **input**: .tsv (from multiple files)
- **output**: .merged.tsv

This script merges multiple TSV files into a single consolidated file, using MergeVcfs from picard/2.27.5. It combines data from all samples, ensuring that variants are aligned correctly and any duplicate entries are handled appropriately.

**18_plink.sh**

- **input**: .tsv
- **output**: .ped and .map

This script prepares the data for genetic association analysis using PLINK. It converts the merged TSV file into PLINK's PED and MAP file formats, which are used for various types of genetic analysis.

**19_K_admixture.sh**

- **input**: .ped and .map
- **output**: .qmatrix and .ematrix

This script performs population structure analysis using ADMIXTURE. It takes the PED and MAP files as input and produces output files that represent the estimated ancestry proportions for each individual, as well as the matrix of ancestry components.

-----
